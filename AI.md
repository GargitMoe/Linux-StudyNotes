## AI基础篇
### 卷积的作用？
1. 局部连接。比起全连接，局部连接会大大减少网络的参数。
2. 权值共享。参数共享也能减少整体参数量。一个卷积核的参数权重被整张图片共享，不会因为图像内位置的不同而改变卷积核内的参数权重。
3. 下采样。下采样能逐渐降低图像分辨率，使得计算资源耗费变少，加速模型训练，也能有效控制过拟合。
### Norm/BN的作用：
神经网络因为是一堆矩阵运算（线性变换）和非线性变化（Relu）的堆叠。
如果 W、b 初始化得不合适，或者上一层输出分布本身不稳定，下一层的输入可能均值飘移或方差过大/过小。（特别是在网络初始的阶段）一层层叠加后，分布会逐渐“失控”。这就是所谓的内部协变量偏移。
N 把输入“拉回”到一个合适的范围（大约均值 0，方差 1），这样能保证梯度比较稳定，也可以帮助收敛。（为什么能帮助收敛？）但是对BN对小batch的效果不好，所以Trans里面会用LayerNorm。
> 顺序：输入 → 卷积/全连接 → BN → ReLU → 下一层 （要先归一化再输入ReLU）
  ```bash
  ls        # 列出文件名
  ls -l     # 列出详细信息（权限、大小、时间）
  ls -a     # 包括隐藏文件
  ls -lh    # 以人类可读的方式显示大小
