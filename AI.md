## AI 基础篇

### 卷积的作用？

1. 局部连接。比起全连接，局部连接会大大减少网络的参数。
2. 权值共享。参数共享也能减少整体参数量。一个卷积核的参数权重被整张图片共享，不会因为图像内位置的不同而改变卷积核内的参数权重。
3. 下采样。下采样能逐渐降低图像分辨率，使得计算资源耗费变少，加速模型训练，也能有效控制过拟合。

### Norm/BN 的作用：

神经网络因为是一堆矩阵运算（线性变换）和非线性变化（Relu）的堆叠。
如果 W、b 初始化得不合适，或者上一层输出分布本身不稳定，下一层的输入可能均值飘移或方差过大/过小。（特别是在网络初始的阶段）一层层叠加后，分布会逐渐“失控”。这就是所谓的内部协变量偏移。
N 把输入“拉回”到一个合适的范围（大约均值 0，方差 1），这样能保证梯度比较稳定，也可以帮助收敛。（为什么能帮助收敛？）但是对 BN 对小 batch 的效果不好，所以 Trans 里面会用 LayerNorm。

> 顺序：输入 → 卷积/全连接 → BN → ReLU → 下一层 （要先归一化再输入 ReLU）

### CNN 的典型输出：

（NxCxHxW）：
batch size x Channel x Hight x Width
(32 x 3 x 1024 x 768)（例子，假设一个 batch 输入 32 张图片）

### 正则化：

#### L2 正则化：

```bash
optimizer = torch.optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-4)
```

这里的 weight decay 实际上就是正则化，作用是防止特别依赖某个权重，惩罚较大的权重
